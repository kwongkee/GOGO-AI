#!/usr/bin/env python3
import warnings

# å¿½ç•¥æ— å…³ç´§è¦çš„è­¦å‘Š
warnings.filterwarnings("ignore", category=UserWarning, module="flask_limiter")
warnings.filterwarnings("ignore", category=UserWarning, module="transformers")
warnings.filterwarnings("ignore", category=FutureWarning)

from flask import Flask, request, jsonify
from flask_limiter import Limiter
from flask_limiter.util import get_remote_address
from langchain_huggingface import HuggingFaceEmbeddings  # æ›´æ–°ä¸ºlangchain-huggingfaceåŒ…
from langchain_community.vectorstores import FAISS
from langchain.chains.retrieval_qa.base import RetrievalQA
from langchain_community.llms import HuggingFaceEndpoint
from langchain_core.runnables import Runnable
from langchain_core.language_models.llms import LLM
from langchain_community.llms import Ollama
import re
from typing import Any, List, Optional, Dict
import requests
import base64
import gzip
import struct
import io
import tempfile
import os
import logging
from PIL import Image
import whisper
import numpy as np
import torch
from transformers import ChineseCLIPModel, ChineseCLIPProcessor  # æ›´æ–°ä¸ºChineseCLIPä¸“ç”¨åŸºç±»
import threading
from datetime import datetime
import shutil
import time
import hashlib
import json
from huggingface_hub import InferenceClient

import sys

print(f"Pythonè·¯å¾„: {sys.executable}")
print(f"å·¥ä½œç›®å½•: {os.getcwd()}")
os.environ['CURL_CA_BUNDLE'] = ''

# å¯¼å…¥é…ç½®æ–‡ä»¶
try:
    from config import config
except ImportError:
    # é»˜è®¤é…ç½®ï¼ˆå¦‚æœé…ç½®æ–‡ä»¶ä¸å­˜åœ¨ï¼‰
    class Config:
        MODEL_CACHE_DIR = "/root/customer_ai/models"
        FAISS_INDEX_DIR = "/root/customer_ai/faiss_index"
        FAISS_BACKUP_DIR = "/root/customer_ai/faiss_backups"
        LOG_DIR = "/root/customer_ai/logs"
        ALLOWED_API_KEYS = ["abc123321cba"]  # é»˜è®¤APIå¯†é’¥
        MAX_REQUESTS_PER_MINUTE = 60
        TEXT_MODEL_PATH = "/root/.cache/huggingface/hub/models--shibing624--text2vec-base-chinese/snapshots/183bb99aa7af74355fb58d16edf8c13ae7c5433e"
        CLIP_MODEL_NAME = "OFA-Sys/chinese-clip-vit-large-patch14"
        WHISPER_MODEL_SIZE = "medium"
        BASE_TEXT_WEIGHT = 0.7
        MIN_TEXT_WEIGHT = 0.5
        MAX_TEXT_WEIGHT = 0.9
        MULTI_ELEMENT_ADJUST = 0.1


    config = Config()

# ç¡®ä¿æ—¥å¿—ç›®å½•å­˜åœ¨
log_dir = "/root/customer_ai/logs"
os.makedirs(log_dir, exist_ok=True)

# é…ç½®æ—¥å¿—ï¼ˆä»…è®°å½•ERRORçº§åˆ«ï¼Œéšè—INFO/WARNINGæ— å…³è¾“å‡ºï¼‰
logging.basicConfig(
    level=logging.DEBUG,  # æ”¹ä¸ºDEBUGçº§åˆ«
    format='%(asctime)s - %(name)s - %(levelname)s - [%(filename)s:%(lineno)d] - %(message)s',
    handlers=[
        logging.FileHandler("/root/customer_ai/logs/hf_proxy_debug.log"),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

app = Flask(__name__)

# é…ç½®é¢‘ç‡é™åˆ¶
limiter = Limiter(
    app=app,
    key_func=get_remote_address,
    default_limits=[f"{config.MAX_REQUESTS_PER_MINUTE} per minute"]
)

# å…¨å±€å˜é‡
clip_model = None
processor = None
whisper_model = None
embeddings = None
llm = None
model_load_lock = threading.Lock()
model_last_health_check = 0
model_health_check_interval = 3600  # 1å°æ—¶æ£€æŸ¥ä¸€æ¬¡

HF_TOKEN = os.getenv("HF_TOKEN", "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")  # ä»ç¯å¢ƒå˜é‡è·å–HF_TOKEN

# åœ¨æ–‡ä»¶é¡¶éƒ¨æ·»åŠ é”å˜é‡å®šä¹‰
_first_request_lock = threading.Lock()
_first_request_done = False
_models_loaded = False

# ==================== åœ¨å…¨å±€å˜é‡åŒº ====================
embeddings = None
global_vectorstore = None
llm = None

@app.before_request
def handle_first_request():
    global _first_request_done
    if not _first_request_done:
        with _first_request_lock:
            if not _first_request_done:
                # æ‰§è¡Œä¸€æ¬¡æ€§åˆå§‹åŒ–
                _init_app()
                _first_request_done = True

    print(f"\n=== æ”¶åˆ°è¯·æ±‚ ===")
    print(f"æ—¶é—´: {datetime.now().isoformat()}")
    print(f"æ–¹æ³•: {request.method}")
    print(f"è·¯å¾„: {request.path}")
    print(f"è¿œç¨‹åœ°å€: {request.remote_addr}")
    print(f"ç”¨æˆ·ä»£ç†: {request.user_agent}")
    print(f"å†…å®¹ç±»å‹: {request.content_type}")
    print(f"å†…å®¹é•¿åº¦: {request.content_length}")

    if request.method == 'POST' and request.content_length:
        try:
            # åªè®°å½•è¯·æ±‚çš„å‰1000ä¸ªå­—ç¬¦ï¼Œé¿å…æ—¥å¿—è¿‡å¤§
            data = request.get_data(as_text=True)
            if len(data) > 1000:
                print(f"è¯·æ±‚æ•°æ®(å‰1000å­—ç¬¦): {data[:1000]}...")
            else:
                print(f"è¯·æ±‚æ•°æ®: {data}")
        except Exception as e:
            print(f"è§£æè¯·æ±‚æ•°æ®å¤±è´¥: {e}")

@app.after_request
def log_response_info(response):
    """è®°å½•æ‰€æœ‰å“åº”çš„ä¿¡æ¯"""
    print(f"=== è¿”å›å“åº” ===")
    print(f"çŠ¶æ€ç : {response.status_code}")
    print(f"å†…å®¹ç±»å‹: {response.content_type}")
    print(f"å†…å®¹é•¿åº¦: {response.content_length}")
    print(f"æ—¶é—´: {datetime.now().isoformat()}\n")
    return response

import asyncio
from concurrent.futures import ThreadPoolExecutor, TimeoutError as FutureTimeoutError
import atexit
import signal


# å…¨å±€çº¿ç¨‹æ± ï¼ˆç¯å¢ƒå˜é‡é…ç½®ï¼‰
MAX_WORKERS = int(os.getenv('RAG_MAX_WORKERS', '6'))
executor = ThreadPoolExecutor(
    max_workers=MAX_WORKERS,
    thread_name_prefix="rag-worker"
)

# ä¼˜é›…å…³é—­
def _graceful_shutdown():
    executor.shutdown(wait=True)
    atexit.register(_graceful_shutdown)
    for sig in (signal.SIGTERM, signal.SIGINT):
        signal.signal(sig, lambda s, f: _graceful_shutdown())

# åŠ¨æ€çº¿ç¨‹æ± ï¼ˆæ ¹æ®è´Ÿè½½è°ƒæ•´ï¼‰
class DynamicThreadPool:
    _instance = None
    _lock = threading.Lock()

    def __new__(cls):
        if cls._instance is None:
            with cls._lock:
                if cls._instance is None:
                    cls._instance = super().__new__(cls)
                    cls._instance._init_pool()
        return cls._instance

    def _init_pool(self):
        self.min_workers = int(os.getenv('MIN_WORKERS', '2'))
        self.max_workers = int(os.getenv('MAX_WORKERS', '8'))
        self.executor = ThreadPoolExecutor(
            max_workers=self.max_workers,
            thread_name_prefix="rag-worker"
        )

        # ä¼˜é›…å…³é—­
        def _shutdown():
            self.executor.shutdown(wait=True)

        atexit.register(_shutdown)
        signal.signal(signal.SIGTERM, lambda s, f: _shutdown())
        signal.signal(signal.SIGINT, lambda s, f: _shutdown())


# å…¨å±€å®ä¾‹
pool = DynamicThreadPool().executor

def run_with_timeout(func, *args, timeout=45, **kwargs):
    loop = asyncio.get_event_loop()
    try:
        return asyncio.wait_for(
            loop.run_in_executor(pool, func, *args, **kwargs),
            timeout=timeout
        )
    except asyncio.TimeoutError:
        raise TimeoutError("LLMè°ƒç”¨è¶…æ—¶")

# å…¨å±€çº¿ç¨‹æ± ï¼ˆå•ä¾‹ï¼Œå¤ç”¨ï¼‰
executor = ThreadPoolExecutor(max_workers=4)
atexit.register(lambda: executor.shutdown(wait=False))

# è‡ªå®šä¹‰LLMç±» - ä½¿ç”¨InferenceClient
class CustomHuggingFaceLLM(LLM, Runnable):
    """ä¿®å¤ï¼šç»§æ‰¿Runnableæ¥å£"""

    def __init__(self, repo_id, token, max_tokens=1024, temperature=0.1):
        self.repo_id = repo_id
        self.token = token
        self.max_tokens = max_tokens
        self.temperature = temperature
        self.model_available = False
        self.client = None
        self._initialize_client()

    @property
    def _llm_type(self) -> str:
        return "custom_huggingface"

    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:
        return self._call_api(prompt)

    def _acall(self, prompt: str, stop: Optional[List[str]] = None) -> str:
        return self._call_api(prompt)

    def _initialize_deepseek_client(self):
        """ä¸“é—¨ä¸º DeepSeek æ¨¡å‹åˆå§‹åŒ–å®¢æˆ·ç«¯ - ä½¿ç”¨å¯¹è¯API"""
        try:
            self.client = InferenceClient(
                model=self.repo_id,
                token=self.token,
                timeout=120
            )

            # ä½¿ç”¨å¯¹è¯APIæµ‹è¯•è¿æ¥
            test_messages = [{"role": "user", "content": "Hello"}]
            test_response = self.client.chat_completion(
                model=self.repo_id,
                messages=test_messages,
                max_tokens=10
            )

            self.model_available = True
            logger.info(f"DeepSeek æ¨¡å‹åˆå§‹åŒ–æˆåŠŸ: {self.repo_id}")

        except Exception as e:
            logger.error(f"âŒ DeepSeek åˆå§‹åŒ–å¤±è´¥: {e}")
            self.model_available = False
            self.client = None

    def _initialize_client(self):
        """åˆå§‹åŒ–DeepSeek-R1å®¢æˆ·ç«¯"""
        try:
            from huggingface_hub import InferenceClient

            # DeepSeek-R1ä¸“ç”¨é…ç½®
            self.client = InferenceClient(
                model=self.repo_id,
                token=self.token,
                timeout=120,
                headers={"Authorization": f"Bearer {self.token}"}
            )

            # æµ‹è¯•è¿æ¥ - ä½¿ç”¨å¯¹è¯API
            test_messages = [{"role": "user", "content": "æµ‹è¯•"}]
            test_response = self.client.chat_completion(
                messages=test_messages,
                max_tokens=10,
                temperature=0.1
            )

            self.model_available = True
            logger.info(f"DeepSeek-R1æ¨¡å‹åˆå§‹åŒ–æˆåŠŸ: {self.repo_id}")

        except Exception as e:
            logger.error(f"DeepSeek-R1åˆå§‹åŒ–å¤±è´¥: {e}")
            self.model_available = False
            # ä½¿ç”¨å¤‡ç”¨æ¨¡å‹
            self._initialize_fallback_model()

    def _initialize_fallback_model(self):
        """åˆå§‹åŒ–å¤‡ç”¨æ¨¡å‹"""
        try:
            # ä½¿ç”¨æ›´ç¨³å®šçš„æ¨¡å‹
            self.client = InferenceClient(
                model="microsoft/DialoGPT-large",
                timeout=60
            )
            self.model_available = True
            logger.info("ä½¿ç”¨å¤‡ç”¨æ¨¡å‹: microsoft/DialoGPT-large")
        except Exception as e:
            logger.error(f"å¤‡ç”¨æ¨¡å‹ä¹Ÿå¤±è´¥: {e}")

    def generate(self, prompts, **kwargs):
        """ç”Ÿæˆæ–‡æœ¬ - å…¼å®¹LangChainæ¥å£"""
        responses = []
        for prompt in prompts:
            try:
                if self.client is None:
                    # é‡æ–°åˆå§‹åŒ–
                    if "deepseek" in self.repo_id.lower():
                        self._initialize_deepseek_client()
                    else:
                        self._initialize_client()

                if not self.model_available:
                    raise Exception("æ¨¡å‹ä¸å¯ç”¨")

                response = self._call_api(prompt)
                responses.append([{"text": response}])
            except Exception as e:
                logger.error(f"APIè°ƒç”¨å¤±è´¥: {e}")
                responses.append([{"text": ""}])
        return {"generations": responses}

    def _call_api(self, prompt):
        """ä½¿ç”¨InferenceClientè°ƒç”¨æ¨¡å‹"""
        try:
            # å¯¹äºDeepSeekæ¨¡å‹ï¼Œåªä½¿ç”¨å¯¹è¯API
            if "deepseek" in self.repo_id.lower():
                return self._call_deepseek_api(prompt)
            else:
                # å¯¹äºå…¶ä»–æ¨¡å‹ï¼Œä½¿ç”¨æ–‡æœ¬ç”Ÿæˆ
                return self._call_text_generation_api(prompt)
        except Exception as e:
            logger.error(f"æ‰€æœ‰APIè°ƒç”¨æ–¹å¼éƒ½å¤±è´¥: {e}")
            raise e

    def _call_deepseek_api(self, prompt):
        """DeepSeek-R1ä¸“ç”¨APIè°ƒç”¨"""
        try:
            messages = [{"role": "user", "content": prompt}]

            response = self.client.chat_completion(
                messages=messages,
                max_tokens=self.max_tokens,
                temperature=self.temperature,
                stream=False
            )

            if response and hasattr(response, 'choices') and len(response.choices) > 0:
                return response.choices[0].message.content
            else:
                raise Exception("DeepSeek APIè¿”å›ç©ºå“åº”")

        except Exception as e:
            logger.error(f"DeepSeek APIè°ƒç”¨å¤±è´¥: {e}")
            raise

    def _call_text_generation_api(self, prompt):
        """é€šç”¨æ–‡æœ¬ç”ŸæˆAPIè°ƒç”¨"""
        try:
            response = self.client.text_generation(
                prompt,
                max_new_tokens=self.max_tokens,
                temperature=self.temperature,
                do_sample=True if self.temperature > 0 else False,
                return_full_text=False
            )

            if response and len(response.strip()) > 0:
                return response.strip()
            else:
                raise Exception("æ–‡æœ¬ç”Ÿæˆè¿”å›ç©ºå“åº”")

        except Exception as e:
            logger.error(f"æ–‡æœ¬ç”ŸæˆAPIè°ƒç”¨å¤±è´¥: {e}")
            raise e


# å¢å¼ºå…¨å±€å¼‚å¸¸å¤„ç†
@app.errorhandler(Exception)
def handle_unexpected_error(error):
    logger.error(f"æœªæ•è·çš„å¼‚å¸¸: {error}", exc_info=True)
    return jsonify({
        "error": "æœåŠ¡å™¨å†…éƒ¨é”™è¯¯",
        "message": str(error),
        "timestamp": datetime.now().isoformat()
    }), 500


# æ·»åŠ è¯¦ç»†æ—¥å¿—ç‚¹
def log_processing_stage(stage, details):
    logger.info(f"[{stage}] {details}")


def call_with_retry(func, max_retries=3, delay=2):
    for attempt in range(max_retries):
        try:
            return func()
        except Exception as e:
            if attempt == max_retries - 1:
                raise e
            time.sleep(delay * (attempt + 1))


# APIå¯†é’¥éªŒè¯è£…é¥°å™¨
def require_api_key(f):
    def decorated_function(*args, **kwargs):
        api_key = request.headers.get('X-API-Key') or request.args.get('api_key')
        if not api_key or api_key not in config.ALLOWED_API_KEYS:
            logger.warning(f"APIå¯†é’¥éªŒè¯å¤±è´¥: {api_key}")
            return jsonify({"error": "æ— æ•ˆçš„APIå¯†é’¥"}), 401
        return f(*args, **kwargs)
    decorated_function.__name__ = f.__name__
    return decorated_function


def load_models():
    global clip_model, processor, whisper_model, embeddings, llm, model_last_health_check

    with model_load_lock:
        try:
            # 1. åŠ è½½åµŒå…¥æ¨¡å‹ï¼ˆæœ€é«˜ä¼˜å…ˆçº§ï¼‰
            if embeddings is None:
                logger.info("åŠ è½½æ–‡æœ¬åµŒå…¥æ¨¡å‹...")
                embeddings = HuggingFaceEmbeddings(
                    model_name="/root/.cache/huggingface/hub/models--shibing624--text2vec-base-chinese/snapshots/183bb99aa7af74355fb58d16edf8c13ae7c5433e",
                    # æœ¬åœ°è·¯å¾„
                    model_kwargs={'device': 'cpu'},
                    encode_kwargs={'normalize_embeddings': True}
                )
                logger.info("æ–‡æœ¬åµŒå…¥æ¨¡å‹åŠ è½½æˆåŠŸ")

            # 2. ç®€åŒ–LLMåˆå§‹åŒ– - ç›´æ¥ä½¿ç”¨å¯é çš„æ¨¡å‹
            if llm is None:
                try:
                    # ä¼˜å…ˆä½¿ç”¨DeepSeek APIï¼ˆéœ€è¦ç½‘ç»œè¿æ¥ï¼‰
                    from langchain_community.llms import HuggingFaceEndpoint

                    llm = HuggingFaceEndpoint(
                        repo_id="deepseek-ai/DeepSeek-R1-0528",
                        task="text-generation",
                        huggingfacehub_api_token=HF_TOKEN,
                        timeout=120,
                        temperature=0.7,
                        max_new_tokens=1024
                    )
                    logger.info("DeepSeek LLMåˆå§‹åŒ–æˆåŠŸ")
                except Exception as e:
                    logger.warning("Hugging Faceå¤±è´¥ï¼Œé™çº§åˆ°Ollama: " . str(e))
                    # å¤‡ç”¨æ–¹æ¡ˆï¼šä½¿ç”¨æœ¬åœ°Ollamaï¼ˆéœ€è¦ç¡®ä¿æœåŠ¡è¿è¡Œï¼‰
                    llm = Ollama(
                        model="llama2",
                        base_url="http://localhost:11434",
                        timeout=60
                    )

            # 3. åŠ è½½CLIPæ¨¡å‹ï¼ˆå¯é€‰ï¼‰
            if clip_model is None:
                try:
                    from transformers import ChineseCLIPModel, ChineseCLIPProcessor
                    clip_model = ChineseCLIPModel.from_pretrained(
                        "OFA-Sys/chinese-clip-vit-large-patch14",
                        cache_dir=config.MODEL_CACHE_DIR
                    )
                    processor = ChineseCLIPProcessor.from_pretrained(
                        "OFA-Sys/chinese-clip-vit-large-patch14",
                        cache_dir=config.MODEL_CACHE_DIR
                    )
                    logger.info("CLIPæ¨¡å‹åŠ è½½æˆåŠŸ")
                except Exception as e:
                    logger.warning(f"CLIPæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
                    clip_model = None
                    processor = None

            # 4. åŠ è½½Whisperæ¨¡å‹ï¼ˆå¯é€‰ï¼‰
            if whisper_model is None:
                try:
                    import whisper
                    whisper_model = whisper.load_model(
                        "base",  # ä½¿ç”¨è¾ƒå°çš„baseæ¨¡å‹
                        download_root=config.MODEL_CACHE_DIR
                    )
                    logger.info("Whisperæ¨¡å‹åŠ è½½æˆåŠŸ")
                except Exception as e:
                    logger.warning(f"Whisperæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
                    whisper_model = None

            # 5. åŠ è½½LLMï¼ˆä½¿ç”¨å¯é çš„å¤‡ç”¨æ–¹æ¡ˆï¼‰
            if llm is None:
                try:
                    # ä½¿ç”¨CustomHuggingFaceLLMåŒ…è£…DeepSeek-R1
                    llm = CustomHuggingFaceLLM(
                        repo_id="deepseek-ai/DeepSeek-R1-0528",
                        token=HF_TOKEN,
                        max_tokens=1024,
                        temperature=0.1
                    )

                    # æ£€æŸ¥æ¨¡å‹æ˜¯å¦çœŸçš„å¯ç”¨
                    if hasattr(llm, 'model_available') and not llm.model_available:
                        raise Exception("DeepSeekæ¨¡å‹åˆå§‹åŒ–å¤±è´¥")

                    logger.info("DeepSeek-R1 LLMåˆå§‹åŒ–æˆåŠŸ")
                except Exception as e:
                    logger.error(f"DeepSeek-R1åŠ è½½å¤±è´¥: {e}")
                    # å¼ºåˆ¶è®¾ç½®ä¸ºNoneï¼Œç¡®ä¿ä½¿ç”¨å¤‡ç”¨æ¨¡å‹
                    llm = None

                    # å¤‡ç”¨æ–¹æ¡ˆ - ä½¿ç”¨æ›´å¯é çš„æ¨¡å‹
                    try:
                        logger.info("å°è¯•ä½¿ç”¨æ›´å¯é çš„æ¨¡å‹...")
                        # ä½¿ç”¨ä¸€ä¸ªå·²çŸ¥å¯ç”¨çš„æ¨¡å‹
                        llm = CustomHuggingFaceLLM(
                            repo_id="microsoft/DialoGPT-large",
                            token=HF_TOKEN,
                            max_tokens=1024,
                            temperature=0.1
                        )

                        if hasattr(llm, 'model_available') and not llm.model_available:
                            raise Exception("å¤‡ç”¨æ¨¡å‹ä¹Ÿä¸å¯ç”¨")

                        logger.info("ä½¿ç”¨å¤‡ç”¨DialoGPT-largeæ¨¡å‹æˆåŠŸ")
                    except Exception as e2:
                        logger.error(f"å¤‡ç”¨æ¨¡å‹ä¹Ÿå¤±è´¥: {e2}")
                        from langchain_community.llms import FakeListLLM
                        llm = FakeListLLM(responses=["æŠ±æ­‰ï¼ŒAIæœåŠ¡æš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·ç¨åå†è¯•ã€‚"])
                        logger.info("ä½¿ç”¨FakeListLLMä½œä¸ºæœ€åå¤‡é€‰")

            model_last_health_check = time.time()
            logger.info("æ‰€æœ‰æ¨¡å‹åŠ è½½å®Œæˆ")

        except Exception as e:
            logger.error(f"æ¨¡å‹åŠ è½½å¼‚å¸¸: {e}")

def check_ollama_health():
    try:
        response = requests.get("http://localhost:11434/api/tags", timeout=5)
        if response.status_code == 200:
            logger.info("Ollamaå¥åº·")
            return True
    except Exception as e:
        logger.error("Ollamaå¥åº·æ£€æŸ¥å¤±è´¥: " . str(e))
        # è‡ªåŠ¨åŒ–é‡å¯ (åŒ¹é…æ–‡æ¡£å®¹é”™)
        os.system("systemctl restart ollama || ollama serve &")
        logger.info("Ollamaé‡å¯å°è¯•å®Œæˆ")
        return False

def check_models_health():
    """æ£€æŸ¥æ¨¡å‹å¥åº·çŠ¶æ€"""
    global model_last_health_check

    current_time = time.time()
    if current_time - model_last_health_check < model_health_check_interval:
        return True

    try:
        # æµ‹è¯•CLIPæ¨¡å‹
        if clip_model and processor:
            test_image = Image.new('RGB', (224, 224), color='red')
            inputs = processor(images=test_image, return_tensors="pt")
            clip_model.get_image_features(**inputs)

        # æµ‹è¯•Whisperæ¨¡å‹
        if whisper_model:
            test_audio = np.zeros((16000,), dtype=np.float32)
            whisper_model.transcribe(test_audio)

        # æµ‹è¯•æ–‡æœ¬åµŒå…¥æ¨¡å‹
        if embeddings:
            test_text = "æµ‹è¯•æ–‡æœ¬"
            embeddings.embed_query(test_text)

        model_last_health_check = current_time
        return True

    except Exception as e:
        logger.error(f"æ¨¡å‹å¥åº·æ£€æŸ¥å¤±è´¥: {e}")
        # å°è¯•é‡æ–°åŠ è½½æ¨¡å‹
        try:
            load_models()
            return True
        except Exception as reload_error:
            logger.error(f"æ¨¡å‹é‡æ–°åŠ è½½å¤±è´¥: {reload_error}")
            return False


load_models()


def generate_vector_multi(text_content, multimodal_elements=[]):
    """å¤šæ¨¡æ€å‘é‡ç”Ÿæˆï¼ˆå¸¦åŠ¨æ€æƒé‡è°ƒæ•´ï¼‰"""
    try:
        # ä½¿ç”¨ç»Ÿä¸€çš„åµŒå…¥æ¨¡å‹
        if text_content and text_content.strip():
            text_embedding = np.array(embeddings.embed_query(text_content))
        else:
            text_embedding = np.zeros(768)

        # å¤šæ¨¡æ€èåˆ
        if multimodal_elements and clip_model and processor:
            multi_embeddings = []
            for elem in multimodal_elements:
                if isinstance(elem, np.ndarray):
                    elem = Image.fromarray(elem)
                inputs = processor(images=elem, return_tensors="pt")
                emb = clip_model.get_image_features(**inputs)[0].detach().numpy()
                multi_embeddings.append(emb)

            if multi_embeddings:
                avg_multi_emb = np.mean(multi_embeddings, axis=0)
                avg_multi_emb = avg_multi_emb / (np.linalg.norm(avg_multi_emb) + 1e-10)

                # åŠ¨æ€æƒé‡è°ƒæ•´ - åŸºäºå¤šæ¨¡æ€å…ƒç´ æ•°é‡
                base_weight = config.BASE_TEXT_WEIGHT
                element_count = len(multimodal_elements)

                # å…ƒç´ è¶Šå¤šï¼Œæ–‡æœ¬æƒé‡è¶Šä½ï¼ˆä½†ä¿æŒåœ¨åˆç†èŒƒå›´å†…ï¼‰
                adjusted_weight = max(
                    config.MIN_TEXT_WEIGHT,
                    min(
                        config.MAX_TEXT_WEIGHT,
                        base_weight - (element_count * config.MULTI_ELEMENT_ADJUST)
                    )
                )

                final_vector = (adjusted_weight * text_embedding + (1 - adjusted_weight) * avg_multi_emb)
                final_vector = final_vector / (np.linalg.norm(final_vector) + 1e-10)
            else:
                final_vector = text_embedding
        else:
            final_vector = text_embedding

        return final_vector.tolist()
    except Exception as e:
        logger.error(f"å¤šæ¨¡æ€å‘é‡ç”Ÿæˆå¤±è´¥: {e}")
        # å›é€€åˆ°çº¯æ–‡æœ¬å‘é‡
        return embeddings.embed_query(text_content if text_content else "ç©ºå†…å®¹")


def backup_faiss_index():
    """å¤‡ä»½FAISSç´¢å¼•"""
    try:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        backup_dir = os.path.join(config.FAISS_BACKUP_DIR, timestamp)
        os.makedirs(backup_dir, exist_ok=True)

        # å¤‡ä»½ç´¢å¼•æ–‡ä»¶
        shutil.copy2(os.path.join(config.FAISS_INDEX_DIR, "index.faiss"),
                     os.path.join(backup_dir, "index.faiss"))
        shutil.copy2(os.path.join(config.FAISS_INDEX_DIR, "index.pkl"),
                     os.path.join(backup_dir, "index.pkl"))

        return True
    except Exception as e:
        logger.error(f"ç´¢å¼•å¤‡ä»½å¤±è´¥: {e}")
        return False


# === æ ¸å¿ƒå‡½æ•°ï¼šåŒæ­¥è°ƒç”¨ LLM ===
def call_llm_with_context_sync(query: str, context: str) -> dict:
    """
    ä½¿ç”¨é¢„è®¡ç®—çš„ä¸Šä¸‹æ–‡ + æŸ¥è¯¢ï¼ŒåŒæ­¥è°ƒç”¨ Ollama LLM ç”Ÿæˆå›ç­”
    """
    if not llm:
        return {
            "result": "AI æœåŠ¡æš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·ç¨åé‡è¯•ã€‚",
            "source_documents": [],
            "status": "error"
        }

    # æ„é€ ä¸“ä¸šæç¤ºè¯
    prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æ¡£åˆ†æåŠ©æ‰‹ã€‚è¯·æ ¹æ®ä»¥ä¸‹ã€å‚è€ƒèµ„æ–™ã€‘ï¼Œç”¨ç®€æ´ã€å‡†ç¡®ã€å‹å¥½çš„ä¸­æ–‡å›ç­”ç”¨æˆ·é—®é¢˜ã€‚

ã€å‚è€ƒèµ„æ–™ã€‘
{context}

ã€ç”¨æˆ·é—®é¢˜ã€‘
{query}

ã€è¦æ±‚ã€‘
1. åªåŸºäºå‚è€ƒèµ„æ–™å›ç­”ï¼Œä¸è¦ç¼–é€ å†…å®¹ã€‚
2. å¦‚æœèµ„æ–™ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·è¯´â€œæ ¹æ®æä¾›çš„æ–‡æ¡£ï¼Œæˆ‘æ— æ³•æ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚â€
3. ç­”æ¡ˆæ§åˆ¶åœ¨ 500 å­—ä»¥å†…ã€‚
4. è¯­è¨€è‡ªç„¶ã€å£è¯­åŒ–ï¼ŒåƒçœŸäººå¯¹è¯ã€‚

å›ç­”ï¼š"""

    try:
        # åŒæ­¥è°ƒç”¨ Ollama
        raw_response = llm.invoke(prompt)

        # æ¸…ç†è¾“å‡ºï¼šç§»é™¤ <think> æ ‡ç­¾ã€å¤šä½™ç©ºæ ¼
        clean_response = re.sub(r'<think>.*?</think>', '', raw_response, flags=re.DOTALL)
        clean_response = re.sub(r'\n\s*\n', '\n', clean_response).strip()

        logger.info(f"LLM åŸå§‹è¾“å‡º: {raw_response[:200]}...")
        logger.info(f"LLM æ¸…ç†åè¾“å‡º: {clean_response}")

        return {
            "result": clean_response,
            "status": "success"
        }

    except Exception as e:
        logger.error(f"LLM è°ƒç”¨å¤±è´¥: {e}")
        return {
            "result": "æŠ±æ­‰ï¼ŒAI åˆ†ææ—¶å‡ºç°äº†ä¸€ç‚¹å°é—®é¢˜ï¼Œè¯·ç¨åé‡è¯•ã€‚",
            "status": "error"
        }


@app.route('/predict', methods=['POST'])
def predict():
    if global_vectorstore is None:
        return jsonify({"error": "ç´¢å¼•æœªå°±ç»ª"}), 503

    data = request.get_json()
    query = data.get('query', '').strip()
    if not query:
        return jsonify({"error": "æŸ¥è¯¢ä¸èƒ½ä¸ºç©º"}), 400

    try:
        # 1. æ£€ç´¢å…¨å±€ç´¢å¼•
        docs = global_vectorstore.similarity_search(query, k=3)
        if not docs:
            return jsonify({
                "answer": "æ ¹æ®æä¾›çš„æ–‡æ¡£ï¼Œæˆ‘æ— æ³•æ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚",
                "source_documents": [],
                "status": "success"
            })

        # 2. æ„å»ºä¸Šä¸‹æ–‡
        context = "\n\n".join([f"èµ„æ–™ {i + 1}: {doc.page_content}" for i, doc in enumerate(docs)])

        # 3. è°ƒç”¨ LLM
        result = call_llm_with_context_sync(query, context)

        # 4. è¿”å›ç»“æ„åŒ–ç»“æœ
        return jsonify({
            "answer": result["result"],
            "source_documents": [doc.page_content[:200] + "..." for doc in docs],
            "source_count": len(docs),
            "status": result["status"],
            "timestamp": datetime.now().isoformat()
        })

    except Exception as e:
        logger.error(f"/predict æ¥å£å¼‚å¸¸: {e}")
        return jsonify({"error": "æŸ¥è¯¢å¤±è´¥ï¼Œè¯·ç¨åé‡è¯•"}), 500


def handle_sync_query(data):
    """å¤„ç†åŒæ­¥æŸ¥è¯¢ï¼ˆä¸Šä¼ å³é—®å³ç­”ï¼‰"""
    query = data['query']
    doc_vectors = data['doc_vectors']
    doc_contents = data['doc_contents']

    # ä½¿ç”¨é¢„è®¡ç®—å‘é‡æ„å»ºä¸´æ—¶FAISSç´¢å¼•
    vector_store = build_faiss_from_precomputed(doc_vectors, doc_contents)

    # æ‰§è¡Œæ£€ç´¢å’Œå›ç­”
    result = execute_qa_with_precomputed(vector_store, query)

    return jsonify({
        "result": result['answer'],
        "source_documents": result['sources'],
        "sync_processed": True,
        "timestamp": datetime.now().isoformat()
    })


def build_faiss_from_precomputed(vectors, contents):
    """ä¿®å¤ç‰ˆçš„FAISSç´¢å¼•æ„å»º - å¢å¼ºé”™è¯¯å¤„ç†"""
    try:
        if not vectors:
            logger.error("å‘é‡æ•°æ®ä¸ºç©º")
            return None

        # éªŒè¯å‘é‡ç»´åº¦ä¸€è‡´æ€§
        dimension = len(vectors[0])
        valid_vectors = []
        for i, vector in enumerate(vectors):
            if vector and len(vector) == dimension:
                valid_vectors.append(vector)
            else:
                logger.warning(f"è·³è¿‡æ— æ•ˆå‘é‡ {i}, ç»´åº¦: {len(vector) if vector else 0}")

        if not valid_vectors:
            logger.error("æ²¡æœ‰æœ‰æ•ˆçš„å‘é‡æ•°æ®")
            return None

        # ä½¿ç”¨numpyæ•°ç»„
        import numpy as np
        vectors_array = np.array(valid_vectors).astype('float32')

        # åˆ›å»ºFAISSç´¢å¼•
        import faiss
        index = faiss.IndexFlatIP(dimension)  # ä½¿ç”¨å†…ç§¯ç›¸ä¼¼åº¦

        # å½’ä¸€åŒ–å‘é‡ï¼ˆæé«˜æ£€ç´¢è´¨é‡ï¼‰
        faiss.normalize_L2(vectors_array)
        index.add(vectors_array)

        # åˆ›å»ºæ–‡æ¡£å­˜å‚¨ï¼ˆä½¿ç”¨ç©ºå†…å®¹æˆ–å ä½ç¬¦ï¼‰
        from langchain.schema import Document
        documents = []
        for i in range(len(valid_vectors)):
            content = contents[i] if i < len(contents) and contents[i] else f"æ–‡æ¡£_{i+1}"
            doc = Document(
                page_content=content,
                metadata={"source": f"doc_{i}", "vector_index": i}
            )
            documents.append(doc)

        # åˆ›å»ºè‡ªå®šä¹‰æ£€ç´¢å™¨
        class PrecomputedFAISS:
            def __init__(self, index, documents, embeddings):
                self.index = index
                self.documents = documents
                self.embeddings = embeddings

            def get_relevant_documents(self, query, k=3):
                try:
                    # ç”ŸæˆæŸ¥è¯¢å‘é‡
                    query_vector = self.embeddings.embed_query(query)
                    query_vector = np.array([query_vector]).astype('float32')
                    faiss.normalize_L2(query_vector)

                    # æœç´¢ç›¸ä¼¼æ–‡æ¡£
                    scores, indices = self.index.search(query_vector, k=min(k, len(self.documents)))

                    results = []
                    for i, idx in enumerate(indices[0]):
                        if 0 <= idx < len(self.documents):
                            results.append(self.documents[idx])

                    return results
                except Exception as e:
                    logger.error(f"FAISSæœç´¢å¤±è´¥: {e}")
                    return self.documents[:k] if self.documents else []

        vector_store = PrecomputedFAISS(index, documents, embeddings)
        logger.info(f"FAISSç´¢å¼•æ„å»ºæˆåŠŸï¼ŒåŒ…å« {len(documents)} ä¸ªæ–‡æ¡£ï¼Œç»´åº¦: {dimension}")
        return vector_store

    except Exception as e:
        logger.error(f"FAISSç´¢å¼•æ„å»ºå¤±è´¥: {e}")
        return None


def create_fallback_index(vectors, contents):
    """åˆ›å»ºé™çº§ç´¢å¼•æ–¹æ¡ˆ"""
    try:
        # ç®€å•çš„å†…å­˜ç´¢å¼•
        class SimpleIndex:
            def __init__(self, vectors, contents):
                self.vectors = vectors
                self.contents = contents
                self.embeddings = embeddings

            def as_retriever(self, search_kwargs=None):
                return self

            def get_relevant_documents(self, query):
                # ç®€å•ç›¸ä¼¼åº¦è®¡ç®—
                query_vector = self.embeddings.embed_query(query)
                similarities = []

                for i, vector in enumerate(self.vectors):
                    if len(vector) == len(query_vector):
                        similarity = np.dot(vector, query_vector) / (
                                np.linalg.norm(vector) * np.linalg.norm(query_vector)
                        )
                        similarities.append((similarity, i))

                similarities.sort(reverse=True)
                top_indices = [idx for _, idx in similarities[:3]]

                from langchain.schema import Document
                return [Document(
                    page_content=self.contents[i],
                    metadata={"source": f"doc_{i}", "similarity": similarities[j][0]}
                ) for j, i in enumerate(top_indices)]

        logger.info("ä½¿ç”¨é™çº§ç´¢å¼•æ–¹æ¡ˆ")
        return SimpleIndex(vectors, contents)

    except Exception as e:
        logger.error(f"é™çº§æ–¹æ¡ˆä¹Ÿå¤±è´¥: {e}")
        return None


def get_chat_history(chat_pid: int):
    """è·å–å†å²å¯¹è¯è®°å½•"""
    try:
        response = requests.post(
            "https://shop.gogo198.cn/collect_website/public/?s=api/getgoods/get_chat_history",
            json={"chat_pid": chat_pid},
            timeout=5
        )
        return response.json().get("history", "æ— å†å²è®°å½•")
    except:
        return "å†å²è®°å½•æœåŠ¡ä¸å¯ç”¨"


@app.route('/refresh_index', methods=['POST'])
@require_api_key
def refresh_index():
    """åˆ·æ–°ç´¢å¼•æ¥å£"""
    try:
        global embeddings
        vectorstore = FAISS.load_local(config.FAISS_INDEX_DIR, embeddings=embeddings,
                                       allow_dangerous_deserialization=True)
        return jsonify({"status": "success", "message": "ç´¢å¼•åˆ·æ–°æˆåŠŸ"})
    except Exception as e:
        return jsonify({"error": str(e)}), 500


@app.route('/health', methods=['GET','POST'])
def health_check():
    """å¥åº·æ£€æŸ¥æ¥å£"""
    models_loaded = all([clip_model, processor, whisper_model, embeddings, llm])
    models_healthy = check_models_health()

    status = "healthy" if (models_loaded and models_healthy) else "degraded"

    return jsonify({
        "status": status,
        "models_loaded": models_loaded,
        "models_healthy": models_healthy,
        "last_health_check": model_last_health_check,
        "timestamp": datetime.now().isoformat(),
        "service": "hf_proxy"
    })


@app.route('/backup_index', methods=['POST'])
@require_api_key
def backup_index():
    """å¤‡ä»½ç´¢å¼•æ¥å£"""
    try:
        success = backup_faiss_index()
        if success:
            return jsonify({"status": "success", "message": "ç´¢å¼•å¤‡ä»½æˆåŠŸ"})
        else:
            return jsonify({"error": "ç´¢å¼•å¤‡ä»½å¤±è´¥"}), 500
    except Exception as e:
        return jsonify({"error": str(e)}), 500


@app.route('/model_status', methods=['GET'])
@require_api_key
def model_status():
    """è·å–æ¨¡å‹è¯¦ç»†çŠ¶æ€"""
    models_info = {
        "clip_model": clip_model is not None,
        "whisper_model": whisper_model is not None,
        "embeddings": embeddings is not None,
        "llm": llm is not None,
        "last_health_check": model_last_health_check,
        "health_check_interval": model_health_check_interval
    }

    return jsonify(models_info)


# é”™è¯¯å¤„ç†
@app.errorhandler(429)
def ratelimit_handler(e):
    return jsonify({"error": "è¯·æ±‚é¢‘ç‡è¿‡é«˜", "message": str(e.description)}), 429


@app.errorhandler(500)
def internal_error_handler(e):
    logger.error(f"æœåŠ¡å™¨å†…éƒ¨é”™è¯¯: {e}")
    return jsonify({"error": "æœåŠ¡å™¨å†…éƒ¨é”™è¯¯"}), 500


def decompress_vectors(compressed_vectors_str, dimension=768):
    """è§£å‹ç¼©æ·±åœ³æœåŠ¡å™¨å‘é€çš„å‘é‡æ•°æ® - æ”¯æŒzlibå’Œgzipæ ¼å¼"""
    try:
        # Base64è§£ç 
        compressed_data = base64.b64decode(compressed_vectors_str)

        # æ£€æµ‹å‹ç¼©æ ¼å¼å¹¶è§£å‹
        if compressed_data.startswith(b'\x1f\x8b'):  # GZIPæ ¼å¼
            decompressed_data = gzip.decompress(compressed_data)
        else:  # ZLIBæ ¼å¼ï¼ˆgzcompressï¼‰
            import zlib
            decompressed_data = zlib.decompress(compressed_data)

        # å°†äºŒè¿›åˆ¶æ•°æ®è½¬æ¢ä¸ºæµ®ç‚¹æ•°åˆ—è¡¨
        num_floats = len(decompressed_data) // 4  # æ¯ä¸ªfloatå 4å­—èŠ‚
        vectors = []
        for i in range(0, num_floats, dimension):
            if i + dimension <= num_floats:
                vec = list(struct.unpack(f'{dimension}f',
                                         decompressed_data[i * 4:(i + dimension) * 4]))
                vectors.append(vec)
        return vectors
    except Exception as e:
        logger.error(f"å‘é‡è§£å‹ç¼©å¤±è´¥: {e}")
        # å°è¯•åŸå§‹gzipè§£å‹ä½œä¸ºå¤‡ç”¨
        try:
            decompressed_data = gzip.decompress(base64.b64decode(compressed_vectors_str))
            # ... å‰©ä½™è§£å‹é€»è¾‘ç›¸åŒ
        except Exception as e2:
            logger.error(f"å¤‡ç”¨è§£å‹æ–¹æ¡ˆä¹Ÿå¤±è´¥: {e2}")
        return []

from prometheus_client import Counter, Histogram, start_http_server
import redis

# å¯åŠ¨ç›‘æ§æœåŠ¡
#start_http_server(8000)

REQUEST_COUNT = Counter('rag_requests_total', 'Total requests')
PROCESSING_TIME = Histogram('rag_processing_seconds', 'Processing time')
ERROR_COUNT = Counter('rag_errors_total', 'Error count', ['type'])
r = redis.Redis(host='localhost', port=6379, db=0)

@app.route('/predict_sync', methods=['POST'])
def predict_sync():
    """
    åŒæ­¥é¢„æµ‹æ¥å£ - å¢å¼ºé”™è¯¯å¤„ç†ç‰ˆæœ¬
    æ³¨æ„ï¼šæ­¤ç«¯ç‚¹ç”±æ·±åœ³æœåŠ¡å™¨è°ƒç”¨ï¼Œå¿…é¡»ä¿è¯å“åº”æ ¼å¼ç¨³å®šã€‚
    """
    # è®°å½•è¯·æ±‚å¼€å§‹ï¼Œç”¨äºæ€§èƒ½è¿½è¸ª
    request_start_time = time.time()
    REQUEST_COUNT.inc()
    logger.info("ğŸ”µ [ç¾å›½æœåŠ¡å™¨] /predict_sync ç«¯ç‚¹æ¥æ”¶åˆ°è¯·æ±‚")

    try:
        # 1. è§£æè¯·æ±‚æ•°æ®
        data = request.get_json()
        if not data:
            logger.error("âŒ è¯·æ±‚æ•°æ®ä¸ºç©ºæˆ–éJSONæ ¼å¼")
            return jsonify({
                "status": "error",
                "error": "æ— æ•ˆçš„è¯·æ±‚æ•°æ®ï¼šå¿…é¡»ä¸ºJSONæ ¼å¼"
            }), 400

        # 2. æå–å‚æ•°
        query = data.get('query', '').strip()
        doc_vectors = data.get('doc_vectors', '')
        chat_id = data.get('chat_id', 'unknown')

        logger.info(f"æ”¶åˆ°æŸ¥è¯¢: '{query[:50]}...', chat_id: {chat_id}")

        # 3. åŸºç¡€å‚æ•°éªŒè¯
        if not query:
            logger.error(f"âŒ æŸ¥è¯¢å†…å®¹ä¸ºç©ºã€‚Chat ID: {chat_id}")
            return jsonify({
                "status": "error",
                "error": "å‚æ•°é”™è¯¯ï¼šæŸ¥è¯¢å†…å®¹ 'query' ä¸èƒ½ä¸ºç©º"
            }), 400

        if not doc_vectors:
            return jsonify({
                "status": "error",
                "error": "å‘é‡æ•°æ®ä¸èƒ½ä¸ºç©º"
            }), 400

        # try:
        #     qa_result = run_with_timeout(
        #         qa_chain.invoke,
        #         {"query": query},
        #         timeout=45
        #     )
        #     PROCESSING_TIME.observe(time.time() - request_start_time)
        #     return jsonify(qa_result)
        # except TimeoutError:
        #     ERROR_COUNT.labels(type='timeout').inc()
        #     # ç¼“å­˜é™çº§
        #     cached = r.get(f"cache:qa:{hash(query)}")
        #     if cached:
        #         return jsonify(json.loads(cached))
        #     return jsonify({"answer": "ç³»ç»Ÿç¹å¿™ï¼Œè¯·ç¨åé‡è¯•", "fallback": True})
        # except Exception as e:
        #     ERROR_COUNT.labels(type='exception').inc()
        #     return jsonify({"error": str(e)}), 500

        # 4. è§£å‹ç¼©å‘é‡æ•°æ®
        logger.info("å¼€å§‹è§£å‹ç¼©å‘é‡æ•°æ®...")
        logger.info(f"å‹ç¼©æ•°æ®é•¿åº¦: {len(doc_vectors)}, å‰100å­—ç¬¦: {doc_vectors[:100]}")

        doc_vectors = decompress_vectors(doc_vectors)

        if not doc_vectors:
            logger.error(f"å‘é‡è§£å‹ç¼©å¤±è´¥ï¼ŒåŸå§‹æ•°æ®: {doc_vectors[:200]}...")
            return jsonify({
                "status": "error",
                "error": "å‘é‡æ•°æ®è§£å‹ç¼©å¤±è´¥"
            }), 400

        logger.info(f"è§£å‹ç¼©æˆåŠŸï¼Œè·å¾— {len(doc_vectors)} ä¸ªå‘é‡")
        if doc_vectors:
            logger.info(f"æ¯ä¸ªå‘é‡ç»´åº¦: {len(doc_vectors[0])}")

        # 4. æ ¸å¿ƒå¤„ç†é€»è¾‘ - ä½¿ç”¨å…¨å±€ç´¢å¼•å’ŒLLMç”Ÿæˆç­”æ¡ˆ
        # é‡è¦ï¼šè¿™é‡Œä½¿ç”¨æ‚¨ç³»ç»Ÿä¸­å·²å®šä¹‰å¥½çš„ qa_chain æˆ–ç±»ä¼¼ç»„ä»¶
        # ç¡®ä¿ global_vectorstore å’Œ llm å·²æ­£ç¡®åˆå§‹åŒ–
        if global_vectorstore is None:
            logger.critical("âŒ å…¨å±€å‘é‡ç´¢å¼•æœªåˆå§‹åŒ–ï¼Œç³»ç»Ÿæœªå°±ç»ª")
            return jsonify({
                "status": "error",
                "error": "ç³»ç»ŸæœåŠ¡æœªå°±ç»ªï¼šå‘é‡ç´¢å¼•åŠ è½½å¤±è´¥"
            }), 503

        # 4.1 æ£€ç´¢ç›¸å…³æ–‡æ¡£
        logger.info("å¼€å§‹å‘é‡æ£€ç´¢...")
        try:
            # ä½¿ç”¨ä¼ å…¥çš„å‘é‡æ„å»ºä¸´æ—¶æ£€ç´¢å™¨ï¼Œæˆ–ä»å…¨å±€ç´¢å¼•æ£€ç´¢
            # æ­¤å¤„æ˜¯å…³é”®ï¼Œå¯èƒ½éœ€è¦æ ¹æ®æ‚¨çš„æ–‡æ¡£å¤„ç†æ–¹å¼è°ƒæ•´
            docs = global_vectorstore.similarity_search(query, k=3)
            if not docs:
                logger.info("æœªæ£€ç´¢åˆ°ç›¸å…³æ–‡æ¡£ï¼Œå°†ä½¿ç”¨ç©ºä¸Šä¸‹æ–‡")
                context = "æœªæ‰¾åˆ°ä¸é—®é¢˜ç›¸å…³çš„å‚è€ƒèµ„æ–™ã€‚"
            else:
                context = "\n\n".join([f"[èµ„æ–™{i+1}] {doc.page_content}" for i, doc in enumerate(docs)])

            logger.info(f"æ£€ç´¢å®Œæˆï¼Œä¸Šä¸‹æ–‡é•¿åº¦: {len(context)} å­—ç¬¦")

        except Exception as retrieval_error:
            logger.error(f"âŒ æ–‡æ¡£æ£€ç´¢é˜¶æ®µå¤±è´¥: {str(retrieval_error)}")
            # é™çº§æ–¹æ¡ˆï¼šä½¿ç”¨ç©ºä¸Šä¸‹æ–‡ï¼Œä¸ç›´æ¥å¤±è´¥
            context = "æ–‡æ¡£æ£€ç´¢ç³»ç»Ÿæš‚æ—¶ä¸å¯ç”¨ã€‚"
            # æˆ–è€…å¯ä»¥é€‰æ‹©ç›´æ¥è¿”å›é”™è¯¯
            # return jsonify({"status": "error", "error": f"æ£€ç´¢å¤±è´¥: {str(retrieval_error)}"}), 500

        # 4.2 æ„å»ºLLMæç¤ºè¯
        prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æ¡£åˆ†æåŠ©æ‰‹ã€‚è¯·æ ¹æ®ä»¥ä¸‹ã€å‚è€ƒèµ„æ–™ã€‘ï¼Œç”¨ç®€æ´ã€å‡†ç¡®ã€å‹å¥½çš„ä¸­æ–‡å›ç­”ç”¨æˆ·é—®é¢˜ã€‚

ã€å‚è€ƒèµ„æ–™ã€‘
{context}

ã€ç”¨æˆ·é—®é¢˜ã€‘
{query}

ã€è¦æ±‚ã€‘
1. åªåŸºäºå‚è€ƒèµ„æ–™å›ç­”ï¼Œä¸è¦ç¼–é€ å†…å®¹ã€‚
2. å¦‚æœèµ„æ–™ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·å¦‚å®å‘ŠçŸ¥ã€‚
3. ç­”æ¡ˆæ§åˆ¶åœ¨300å­—ä»¥å†…ã€‚
4. è¯­è¨€è‡ªç„¶ã€å£è¯­åŒ–ã€‚

å›ç­”ï¼š"""

        logger.info("å¼€å§‹è°ƒç”¨LLMç”Ÿæˆå›ç­”...")

        # 4.3 è°ƒç”¨LLM - è¿™æ˜¯æœ€å¯èƒ½è¶…æ—¶æˆ–å‡ºé”™çš„éƒ¨åˆ†
        try:
            # ä½¿ç”¨æ‚¨ç³»ç»Ÿä¸­åˆå§‹åŒ–å¥½çš„ llm å¯¹è±¡
            # å…³é”®ï¼šè¿™é‡Œè®¾ç½®äº† timeout=60.0ï¼Œç¡®ä¿Ollamaè°ƒç”¨ä¸ä¼šæ— é™æœŸæŒ‚èµ·
            if llm is None:
                raise Exception("LLMæœåŠ¡æœªåˆå§‹åŒ–")

            # è°ƒç”¨LLMç”Ÿæˆå›ç­”
            llm_response = llm.invoke(prompt)  # æˆ–ä½¿ç”¨ llm.generate(), æ ¹æ®æ‚¨çš„LangChainç‰ˆæœ¬è°ƒæ•´

            # æ¸…ç†å“åº”å†…å®¹ï¼Œç§»é™¤å¯èƒ½çš„æ ‡è®°
            import re
            clean_response = re.sub(r'<think>.*?</think>', '', llm_response, flags=re.DOTALL).strip()
            clean_response = re.sub(r'\n\s*\n', '\n', clean_response)

            if not clean_response or len(clean_response) < 5:
                clean_response = "æŠ±æ­‰ï¼Œæˆ‘æ— æ³•åŸºäºç°æœ‰èµ„æ–™ç”Ÿæˆæœ‰æ•ˆçš„å›ç­”ã€‚"

            logger.info(f"âœ… LLMè°ƒç”¨æˆåŠŸï¼Œç”Ÿæˆå›ç­”é•¿åº¦: {len(clean_response)}")

        except Exception as llm_error:
            # ç‰¹åˆ«æ•è·LLMè°ƒç”¨ç›¸å…³çš„å¼‚å¸¸ï¼ˆå¦‚è¶…æ—¶ã€è¿æ¥å¤±è´¥ï¼‰
            error_msg = str(llm_error)
            logger.error(f"âŒ LLMè°ƒç”¨å¤±è´¥: {error_msg}")

            # æ ¹æ®é”™è¯¯ç±»å‹æä¾›æ›´å‹å¥½çš„æç¤º
            if "timeout" in error_msg.lower() or "timed out" in error_msg.lower():
                error_msg_user = "AIæ€è€ƒè¶…æ—¶ï¼Œè¯·ç®€åŒ–é—®é¢˜æˆ–ç¨åé‡è¯•ã€‚"
            elif "connection" in error_msg.lower() or "connect" in error_msg.lower():
                error_msg_user = "AIæœåŠ¡æš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–ç¨åé‡è¯•ã€‚"
            else:
                error_msg_user = "AIå¤„ç†é‡åˆ°æ„å¤–é”™è¯¯ã€‚"

            # è¿”å›é”™è¯¯ä¿¡æ¯ï¼Œè®©æ·±åœ³æœåŠ¡å™¨èƒ½è¿›è¡Œé™çº§å¤„ç†
            return jsonify({
                "status": "error",
                "error": error_msg_user,
                "internal_error": error_msg[:200]  # è®°å½•å†…éƒ¨é”™è¯¯å‰200å­—ç¬¦ä¾›è°ƒè¯•
            }), 500

        # 5. æ„å»ºæˆåŠŸå“åº”
        processing_time = round(time.time() - request_start_time, 2)
        logger.info(f"âœ… /predict_sync è¯·æ±‚å¤„ç†å®Œæˆï¼Œæ€»è€—æ—¶: {processing_time}ç§’")

        response_data = {
            "status": "success",
            "result": clean_response,
            "source_documents": [doc.page_content[:100] + "..." for doc in docs] if docs else [],
            "processing_time_seconds": processing_time,
            "vector_count": len(doc_vectors),
            "retrieved_docs_count": len(docs)
        }

        return jsonify(response_data)

    except json.JSONDecodeError as e:
        # æ•è·JSONè§£æé”™è¯¯ï¼ˆæœ€é¡¶å±‚çš„è¯·æ±‚æ•°æ®é—®é¢˜ï¼‰
        logger.error(f"âŒ è¯·æ±‚JSONè§£æå¤±è´¥: {str(e)}")
        return jsonify({
            "status": "error",
            "error": "è¯·æ±‚æ•°æ®æ ¼å¼é”™è¯¯ï¼šå¿…é¡»æ˜¯æœ‰æ•ˆçš„JSON"
        }), 400

    except Exception as e:
        # æ•è·æ‰€æœ‰å…¶ä»–æœªé¢„æ–™çš„å¼‚å¸¸
        error_msg = str(e)
        logger.error(f"âŒ /predict_sync å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿæœªé¢„æ–™é”™è¯¯: {error_msg}")
        import traceback
        logger.error(f"è¯¦ç»†å †æ ˆä¿¡æ¯: {traceback.format_exc()}")

        # è¿”å›é€šç”¨é”™è¯¯ä¿¡æ¯ï¼Œé¿å…æ³„éœ²å†…éƒ¨ç»†èŠ‚
        return jsonify({
            "status": "error",
            "error": "æœåŠ¡å™¨å†…éƒ¨å¤„ç†å¼‚å¸¸",
            "request_id": f"req_{hash(str(time.time()))}"  # ç®€å•çš„è¯·æ±‚IDï¼Œç”¨äºæ—¥å¿—è¿½è¸ª
        }), 500



def process_sync_task(task_id):
    """å¼‚æ­¥å¤„ç†åŒæ­¥ä»»åŠ¡"""
    try:
        redis_client = get_redis_connection()

        # è·å–ä»»åŠ¡æ•°æ®
        task_data = redis_client.hgetall(f"sync_task:{task_id}")
        if not task_data:
            return

        query = task_data['query']
        doc_vectors = json.loads(task_data['doc_vectors'])
        chat_id = task_data['chat_id']

        # æ‰§è¡Œå®é™…çš„æ£€ç´¢å’Œç”Ÿæˆ
        result = actual_retrieval_and_generation(query, doc_vectors, chat_id)

        # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºå®Œæˆ
        redis_client.hset(f"sync_task:{task_id}", 'status', 'completed')
        redis_client.hset(f"sync_task:{task_id}", 'result', json.dumps(result))
        redis_client.hset(f"sync_task:{task_id}", 'complete_time', time.time())

    except Exception as e:
        logger.error(f"å¤„ç†åŒæ­¥ä»»åŠ¡å¤±è´¥: {e}")
        redis_client.hset(f"sync_task:{task_id}", 'status', 'failed')
        redis_client.hset(f"sync_task:{task_id}", 'error', str(e))


@app.route('/sync_task_status/<task_id>', methods=['GET'])
def sync_task_status(task_id):
    """æŸ¥è¯¢åŒæ­¥ä»»åŠ¡çŠ¶æ€"""
    redis_client = get_redis_connection()
    task_data = redis_client.hgetall(f"sync_task:{task_id}")

    if not task_data:
        return jsonify({'status': 'error', 'error': 'ä»»åŠ¡ä¸å­˜åœ¨'}), 404

    response = {
        'task_id': task_id,
        'status': task_data.get('status', 'unknown')
    }

    if task_data.get('status') == 'completed':
        response['result'] = json.loads(task_data.get('result', '{}'))
    elif task_data.get('status') == 'failed':
        response['error'] = task_data.get('error', '')

    return jsonify(response)



@app.route('/debug', methods=['GET'])
def debug_info():
    """è°ƒè¯•ä¿¡æ¯ç«¯ç‚¹"""
    info = {
        "service": "hf_proxy",
        "status": "running",
        "timestamp": datetime.now().isoformat(),
        "components": {
            "embeddings": "loaded" if embeddings else "missing",
            "llm": "loaded" if llm else "missing",
            "global_vectorstore": "loaded" if global_vectorstore else "missing"
        },
        "endpoints": [
            "/health",
            "/predict_sync",
            "/debug"
        ]
    }
    return jsonify(info)


@app.route('/test_predict_sync', methods=['POST'])
def test_predict_sync():
    """æµ‹è¯•/predict_syncç«¯ç‚¹"""
    test_data = {
        "query": "æµ‹è¯•é—®é¢˜",
        "doc_vectors": [[0.1] * 768],  # æµ‹è¯•å‘é‡
        "doc_contents": ["è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æ–‡æ¡£å†…å®¹"]
    }

    try:
        result = handle_real_sync_query(
            test_data["query"],
            test_data["doc_vectors"],
            test_data["doc_contents"]
        )
        return jsonify({
            "test_status": "success",
            "result": result
        })
    except Exception as e:
        return jsonify({
            "test_status": "failed",
            "error": str(e)
        }), 500

def handle_real_sync_query(query, doc_vectors, doc_contents):
    """å®é™…å¤„ç†åŒæ­¥æŸ¥è¯¢ï¼ˆå¢å¼ºç‰ˆæœ¬ï¼‰"""
    try:
        logger.info("å¼€å§‹å¤„ç†çœŸå®åŒæ­¥æŸ¥è¯¢")

        # 1. éªŒè¯è¾“å…¥æ•°æ®
        if not query or not isinstance(query, str):
            logger.error("æŸ¥è¯¢å†…å®¹æ— æ•ˆ")
            return {
                "answer": "æŸ¥è¯¢å†…å®¹æ ¼å¼é”™è¯¯",
                "source_documents": [],
                "status": "error"
            }

        # 2. æ„å»ºä¸´æ—¶ç´¢å¼•ï¼ˆå¢å¼ºé”™è¯¯å¤„ç†ï¼‰
        vector_store = None
        try:
            vector_store = build_faiss_from_precomputed(doc_vectors, doc_contents)
            if vector_store:
                logger.info("ä¸´æ—¶FAISSç´¢å¼•æ„å»ºæˆåŠŸ")
            else:
                logger.warning("ä¸´æ—¶ç´¢å¼•æ„å»ºå¤±è´¥ï¼Œä½¿ç”¨é™çº§æ–¹æ¡ˆ")
        except Exception as e:
            logger.error(f"ç´¢å¼•æ„å»ºå¼‚å¸¸: {e}")

        # 3. æ£€ç´¢ç›¸å…³æ–‡æ¡£
        source_docs = []
        if vector_store:
            try:
                source_docs = vector_store.get_relevant_documents(query)
                logger.info(f"æ£€ç´¢åˆ° {len(source_docs)} ä¸ªç›¸å…³æ–‡æ¡£")
            except Exception as e:
                logger.error(f"æ–‡æ¡£æ£€ç´¢å¤±è´¥: {e}")
                source_docs = []

        # 4. æ„é€ ä¸Šä¸‹æ–‡
        context = ""
        if source_docs:
            context = "\n\n".join([doc.page_content for doc in source_docs])
        elif doc_contents:
            # ä½¿ç”¨å‰3ä¸ªæ–‡æ¡£å†…å®¹ä½œä¸ºä¸Šä¸‹æ–‡
            context = "\n\n".join(doc_contents[:3])
            logger.info("ä½¿ç”¨ç›´æ¥æ–‡æ¡£å†…å®¹ä½œä¸ºä¸Šä¸‹æ–‡")
        else:
            context = "æ— å¯ç”¨æ–‡æ¡£å†…å®¹"
            logger.warning("æ²¡æœ‰å¯ç”¨çš„æ–‡æ¡£å†…å®¹")

        # 5. LLMè°ƒç”¨ï¼ˆå¢å¼ºé‡è¯•æœºåˆ¶ï¼‰
        answer = call_llm_with_retry(query, context)

        return {
            "answer": answer,
            "source_documents": [doc.page_content[:200] + "..." for doc in source_docs] if source_docs else [],
            "status": "success"
        }

    except Exception as e:
        logger.error(f"åŒæ­¥æŸ¥è¯¢å¤„ç†å¤±è´¥: {e}", exc_info=True)
        return {
            "answer": "ç³»ç»Ÿå¤„ç†å¼‚å¸¸ï¼Œè¯·ç¨åé‡è¯•",
            "source_documents": [],
            "status": "error"
        }


def call_llm_with_retry(query, context, max_retries=3):
    """å¸¦é‡è¯•çš„LLMè°ƒç”¨"""
    for attempt in range(max_retries):
        try:
            prompt = f"""åŸºäºä»¥ä¸‹æ–‡æ¡£å†…å®¹å›ç­”é—®é¢˜ã€‚

æ–‡æ¡£å†…å®¹ï¼š
{context}

é—®é¢˜ï¼š{query}

è¯·æ ¹æ®æ–‡æ¡£å†…å®¹æä¾›å‡†ç¡®å›ç­”ã€‚å¦‚æœæ–‡æ¡£ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·å¦‚å®å‘ŠçŸ¥ã€‚"""

            if hasattr(llm, 'invoke'):
                response = llm.invoke(prompt)
            elif hasattr(llm, '__call__'):
                response = llm(prompt)
            else:
                response = "LLMæœåŠ¡ä¸å¯ç”¨"

            if response and len(response.strip()) > 10:
                return response

            logger.warning(f"LLMè¿”å›ç©ºæˆ–è¿‡çŸ­å“åº”ï¼Œå°è¯• {attempt + 1}/{max_retries}")
            time.sleep(2)  # ç­‰å¾…2ç§’åé‡è¯•

        except Exception as e:
            logger.error(f"LLMè°ƒç”¨å¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}): {e}")
            if attempt == max_retries - 1:
                return "æŠ±æ­‰ï¼ŒAIæœåŠ¡æš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·ç¨åå†è¯•ã€‚"
            time.sleep(2)

    return "ç³»ç»Ÿç¹å¿™ï¼Œè¯·ç¨åé‡è¯•ã€‚"

@app.route('/llm_health', methods=['GET'])
def llm_health_check():
    """æ£€æŸ¥LLMæœåŠ¡å¥åº·çŠ¶æ€"""
    try:
        # æµ‹è¯•LLMæ˜¯å¦å¯ç”¨
        test_prompt = "æµ‹è¯•"
        if hasattr(llm, 'generate'):
            result = llm.generate([test_prompt])
            status = "healthy"
        else:
            result = llm(test_prompt)
            status = "healthy" if result and len(result) > 0 else "unhealthy"

        return jsonify({
            "status": status,
            "llm_type": str(type(llm)),
            "timestamp": datetime.now().isoformat()
        })
    except Exception as e:
        return jsonify({
            "status": "unhealthy",
            "error": str(e),
            "llm_type": str(type(llm)),
            "timestamp": datetime.now().isoformat()
        }), 503


# æ·»åŠ ä¸€ä¸ªæ ¹è·¯å¾„æµ‹è¯•
@app.route('/', methods=['GET'])
def root_test():
    logger.info("æ ¹è·¯å¾„è¢«è®¿é—®")
    return jsonify({
        "service": "hf_proxy",
        "status": "running",
        "endpoints": ["/health", "/predict_sync", "/"],
        "timestamp": datetime.now().isoformat()
    })


def fallback_similarity_search(query, doc_contents, doc_vectors):
    """å¤‡ç”¨ç›¸ä¼¼åº¦æœç´¢æ–¹æ¡ˆï¼ˆåŒæ­¥ç‰ˆæœ¬ï¼‰"""
    try:
        # è®¡ç®—æŸ¥è¯¢å‘é‡
        query_vector = embeddings.embed_query(query)

        # è®¡ç®—ç›¸ä¼¼åº¦
        similarities = []
        for i, doc_vector in enumerate(doc_vectors):
            if len(doc_vector) == len(query_vector):
                similarity = np.dot(query_vector, doc_vector) / (
                        np.linalg.norm(query_vector) * np.linalg.norm(doc_vector)
                )
                similarities.append((similarity, i))

        # æ’åºå–å‰3
        similarities.sort(reverse=True)
        top_indices = [idx for _, idx in similarities[:3]]

        # æ„å»ºä¸Šä¸‹æ–‡
        context = "\n\n".join([doc_contents[i] for i in top_indices])

        # åŒæ­¥è°ƒç”¨LLM
        return call_llm_with_context_sync(query, context)

    except Exception as e:
        logger.error(f"å¤‡ç”¨æ–¹æ¡ˆå¤±è´¥: {e}")
        return jsonify({"error": "æ‰€æœ‰å¤„ç†æ–¹æ¡ˆå‡å¤±è´¥"}), 500


@app.route('/debug_models', methods=['GET'])
def debug_models():
    """æ¨¡å‹è°ƒè¯•æ¥å£"""
    models_status = {
        "embeddings_loaded": embeddings is not None,
        "llm_loaded": llm is not None,
        "clip_loaded": clip_model is not None,
        "whisper_loaded": whisper_model is not None,
        "embeddings_type": str(type(embeddings)) if embeddings else None,
        "llm_type": str(type(llm)) if llm else None
    }

    # æµ‹è¯•åµŒå…¥æ¨¡å‹
    if embeddings:
        try:
            test_embedding = embeddings.embed_query("æµ‹è¯•æ–‡æœ¬")
            models_status["embeddings_working"] = True
            models_status["embedding_dim"] = len(test_embedding)
        except Exception as e:
            models_status["embeddings_working"] = False
            models_status["embeddings_error"] = str(e)

    # æµ‹è¯•LLM
    if llm:
        try:
            # å¯¹äºCustomHuggingFaceLLM
            if hasattr(llm, 'model_available'):
                models_status["llm_working"] = llm.model_available
            else:
                # ç®€å•æµ‹è¯•
                test_response = llm.generate(["Hello"])
                models_status["llm_working"] = True
        except Exception as e:
            models_status["llm_working"] = False
            models_status["llm_error"] = str(e)

    return jsonify(models_status)


# ==================== å¯åŠ¨æ—¶åŠ è½½ï¼ˆæ”¾åœ¨æ–‡ä»¶æœ«å°¾ï¼Œapp.run ä¹‹å‰ï¼‰ ====================
def _init_app():
    global embeddings, global_vectorstore, llm

    logger.info("å¼€å§‹åˆå§‹åŒ–åº”ç”¨ç»„ä»¶...")

    try:
        # 1. æ–‡æœ¬åµŒå…¥æ¨¡å‹
        logger.info("åŠ è½½æ–‡æœ¬åµŒå…¥æ¨¡å‹...")
        embeddings = HuggingFaceEmbeddings(model_name=config.TEXT_MODEL_PATH)
        logger.info("æ–‡æœ¬åµŒå…¥æ¨¡å‹åŠ è½½å®Œæˆ")

        # 2. å…¨å±€FAISSç´¢å¼•
        index_path = "/customer_ai/faiss_index"
        logger.info(f"åŠ è½½FAISSç´¢å¼•ä»: {index_path}")
        if os.path.exists(os.path.join(index_path, "index.faiss")):
            global_vectorstore = FAISS.load_local(
                index_path, embeddings, allow_dangerous_deserialization=True
            )
            logger.info("å…¨å±€FAISSç´¢å¼•åŠ è½½æˆåŠŸ")
        else:
            global_vectorstore = FAISS.from_documents([], embeddings)
            logger.info("åˆ›å»ºç©ºå…¨å±€ç´¢å¼•")

        # 3. ç®€åŒ–LLMåˆå§‹åŒ–ï¼ˆåŸºäºç°æœ‰æˆåŠŸä»£ç ï¼‰
        logger.info("åˆå§‹åŒ–LLMæœåŠ¡...")
        llm = _initialize_simple_ollama()

        logger.info("åº”ç”¨ç»„ä»¶åˆå§‹åŒ–å…¨éƒ¨å®Œæˆ")

    except Exception as e:
        logger.error(f"åº”ç”¨åˆå§‹åŒ–å¼‚å¸¸: {e}")
        raise

def _initialize_simple_ollama():
    """ç®€åŒ–ç‰ˆOllamaåˆå§‹åŒ–ï¼ˆåŸºäºç°æœ‰æˆåŠŸä»£ç ï¼‰"""
    try:
        # ç›´æ¥ä½¿ç”¨å·²çŸ¥å¯ç”¨çš„æ¨¡å‹
        from langchain_community.llms import Ollama
        llm = Ollama(
            model="llama2",  # æ–‡æ¡£4æ˜¾ç¤ºè¿™ä¸ªæ¨¡å‹å¯ç”¨
            base_url="http://localhost:11434",
            timeout=60
        )
        logger.info("Ollama LLMåˆå§‹åŒ–æˆåŠŸ")
        return llm
    except Exception as e:
        logger.error(f"Ollamaåˆå§‹åŒ–å¤±è´¥: {e}")
        # ä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆ
        from langchain_community.llms import FakeListLLM
        return FakeListLLM(responses=["æ ¹æ®æ–‡æ¡£å†…å®¹ï¼Œè¿™æ˜¯ä¸€ä¸ªéœ€è¦è¿›ä¸€æ­¥åˆ†æçš„å¤æ‚é—®é¢˜ã€‚"])

def _initialize_ollama_with_fallback(self):
    """åŠ¨æ€æ£€æµ‹å¹¶åˆå§‹åŒ–Ollamaï¼Œå¸¦å¤šçº§é™çº§ç­–ç•¥"""
    try:
        # é¦–å…ˆæ£€æµ‹ç³»ç»Ÿå¯ç”¨çš„Ollamaæ¨¡å‹
        available_models = _get_available_ollama_models()  # âœ… ä¿®å¤ï¼šç§»é™¤self

        if not available_models:
            logger.warning("æœªæ‰¾åˆ°ä»»ä½•å¯ç”¨çš„Ollamaæ¨¡å‹")
            return _create_fallback_llm()  # âœ… ä¿®å¤ï¼šç§»é™¤self

        # æŒ‰ä¼˜å…ˆçº§å°è¯•ä¸åŒçš„æ¨¡å‹
        model_priority = [
            "llama3.2:3b",  # é¦–é€‰æ¨¡å‹
            "llama2",  # å®é™…å­˜åœ¨çš„æ¨¡å‹
            "llama2:latest",  # å®Œæ•´åç§°
            "mistral",  # å¤‡ç”¨æ¨¡å‹1
            "gemma",  # å¤‡ç”¨æ¨¡å‹2
            available_models[0]  # ä½¿ç”¨ç¬¬ä¸€ä¸ªå¯ç”¨æ¨¡å‹
        ]

        for model_name in model_priority:
            if model_name in available_models:
                try:
                    logger.info(f"å°è¯•åŠ è½½æ¨¡å‹: {model_name}")
                    llm = Ollama(model=model_name, base_url="http://localhost:11434")

                    # æµ‹è¯•æ¨¡å‹æ˜¯å¦çœŸçš„å¯ç”¨
                    test_response = llm.invoke("æµ‹è¯•")
                    if test_response and len(test_response.strip()) > 0:
                        logger.info(f"Ollamaæ¨¡å‹åŠ è½½æˆåŠŸ: {model_name}")
                        return llm
                    else:
                        logger.warning(f"æ¨¡å‹å“åº”å¼‚å¸¸: {model_name}")
                except Exception as e:
                    logger.warning(f"æ¨¡å‹åŠ è½½å¤±è´¥ {model_name}: {e}")
                    continue

        # æ‰€æœ‰æ¨¡å‹å°è¯•éƒ½å¤±è´¥ï¼Œä½¿ç”¨é™çº§æ–¹æ¡ˆ
        logger.warning("æ‰€æœ‰Ollamaæ¨¡å‹å°è¯•å¤±è´¥ï¼Œä½¿ç”¨é™çº§LLM")
        return _create_fallback_llm()  # âœ… ä¿®å¤ï¼šç§»é™¤self

    except Exception as e:
        logger.error(f"Ollamaåˆå§‹åŒ–å¼‚å¸¸: {e}")
        return _create_fallback_llm()  # âœ… ä¿®å¤ï¼šç§»é™¤self


def _get_available_ollama_models(self):
    """è·å–ç³»ç»Ÿå¯ç”¨çš„Ollamaæ¨¡å‹åˆ—è¡¨"""
    try:
        # æ‰§è¡Œollama listå‘½ä»¤è·å–æ¨¡å‹åˆ—è¡¨
        import subprocess
        result = subprocess.run(['ollama', 'list'],
                                capture_output=True, text=True, timeout=30)

        if result.returncode == 0:
            models = []
            lines = result.stdout.strip().split('\n')
            for line in lines[1:]:  # è·³è¿‡æ ‡é¢˜è¡Œ
                if line.strip():
                    parts = line.split()
                    if parts:
                        models.append(parts[0])
            logger.info(f"æ£€æµ‹åˆ°å¯ç”¨Ollamaæ¨¡å‹: {models}")
            return models
        else:
            logger.warning("è·å–Ollamaæ¨¡å‹åˆ—è¡¨å¤±è´¥")
            return []

    except Exception as e:
        logger.warning(f"è·å–Ollamaæ¨¡å‹åˆ—è¡¨å¼‚å¸¸: {e}")
        return []

def _create_fallback_llm(self):
    """åˆ›å»ºå¤šçº§é™çº§LLMæ–¹æ¡ˆ"""
    try:
        # ç¬¬ä¸€çº§é™çº§ï¼šä½¿ç”¨HuggingFace API
        try:
            from langchain_community.llms import HuggingFaceEndpoint
            llm = HuggingFaceEndpoint(
                repo_id="HuggingFaceH4/zephyr-7b-beta",
                task="text-generation",
                model_kwargs={
                    "max_new_tokens": 512,
                    "temperature": 0.1
                }
            )
            logger.info("ä½¿ç”¨HuggingFace APIä½œä¸ºé™çº§æ–¹æ¡ˆ")
            return llm
        except Exception as e:
            logger.warning(f"HuggingFace APIé™çº§å¤±è´¥: {e}")

        # ç¬¬äºŒçº§é™çº§ï¼šä½¿ç”¨ç®€å•çš„æœ¬åœ°æ¨¡å‹
        try:
            from langchain_community.llms import CTransformers
            llm = CTransformers(
                model="TheBloke/Llama-2-7B-Chat-GGML",
                model_file="llama-2-7b-chat.ggmlv3.q4_0.bin",
                model_type="llama"
            )
            logger.info("ä½¿ç”¨æœ¬åœ°CTransformersä½œä¸ºé™çº§æ–¹æ¡ˆ")
            return llm
        except Exception as e:
            logger.warning(f"æœ¬åœ°æ¨¡å‹é™çº§å¤±è´¥: {e}")

        # æœ€ç»ˆé™çº§ï¼šæç®€å›ç­”ç”Ÿæˆå™¨
        class MinimalFallbackLLM:
            def invoke(self, prompt):
                responses = [
                    "æ ¹æ®æ‚¨æä¾›çš„æ–‡æ¡£ï¼Œè¿™æ˜¯ä¸€ä¸ªéœ€è¦åˆ†æçš„å†…å®¹ã€‚",
                    "æˆ‘å·²æ”¶åˆ°æ‚¨çš„æŸ¥è¯¢ï¼Œæ­£åœ¨å¤„ç†ä¸­ã€‚",
                    "åŸºäºä¸Šä¼ çš„æ–‡æ¡£ä¿¡æ¯ï¼Œå»ºè®®è¿›ä¸€æ­¥åˆ†æã€‚",
                    "æ–‡æ¡£å†…å®¹å·²æ¥æ”¶ï¼Œéœ€è¦æ—¶é—´å¤„ç†ã€‚"
                ]
                return responses[hash(prompt) % len(responses)]

        logger.info("ä½¿ç”¨æç®€å¤‡ç”¨LLM")
        return MinimalFallbackLLM()

    except Exception as e:
        logger.error(f"æ‰€æœ‰é™çº§æ–¹æ¡ˆéƒ½å¤±è´¥: {e}")

        # ä¿è¯è‡³å°‘æœ‰ä¸€ä¸ªå¯ç”¨çš„LLM
        class GuaranteedLLM:
            def invoke(self, prompt):
                return "ç³»ç»Ÿæ­£åœ¨å¤„ç†æ‚¨çš„è¯·æ±‚ï¼Œè¯·ç¨åæŸ¥çœ‹ç»“æœã€‚"

        return GuaranteedLLM()

def main():
    """ä¸»å¯åŠ¨å‡½æ•°"""
    try:
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(config.MODEL_CACHE_DIR, exist_ok=True)
        os.makedirs(config.FAISS_INDEX_DIR, exist_ok=True)
        os.makedirs(config.FAISS_BACKUP_DIR, exist_ok=True)
        os.makedirs(config.LOG_DIR, exist_ok=True)

        # å¼ºåˆ¶åˆå§‹åŒ–åº”ç”¨
        logger.info("=== å¼€å§‹å¼ºåˆ¶åˆå§‹åŒ–åº”ç”¨ ===")
        _init_app()

        # éªŒè¯å…³é”®ç»„ä»¶
        if not all([embeddings, global_vectorstore, llm]):
            logger.error("å…³é”®ç»„ä»¶åˆå§‹åŒ–å¤±è´¥")
            return False

        logger.info("=== åº”ç”¨åˆå§‹åŒ–å®Œæˆï¼Œå¯åŠ¨FlaskæœåŠ¡ ===")
        return True

    except Exception as e:
        logger.error(f"å¯åŠ¨å¤±è´¥: {e}")
        return False

if __name__ == '__main__':
    if main():
        # æ·»åŠ å¯åŠ¨æˆåŠŸæ—¥å¿—
        logger.info(f"FlaskæœåŠ¡å¯åŠ¨åœ¨ 0.0.0.0:5000")
        print("=== FlaskæœåŠ¡å¯åŠ¨æˆåŠŸ ===")
        print("å¥åº·æ£€æŸ¥åœ°å€: http://0.0.0.0:5000/health")

        def periodic_health_check():
            check_ollama_health()
            threading.Timer(60, periodic_health_check).start()

        periodic_health_check()

        app.run(host='0.0.0.0', port=5000, debug=False)
    else:
        logger.error("åº”ç”¨å¯åŠ¨å¤±è´¥")
        sys.exit(1)